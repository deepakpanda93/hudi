{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4902a7a7-c239-4700-a95f-b4daadc74b5b",
   "metadata": {},
   "source": [
    "# Deep Dive into Hudi Metadata Table and Indexing Enhancements in 1.x, Including SQL-Based Index Management\n",
    "\n",
    "Welcome to this guide on the Hudi Metadata Table and its role in boosting performance. In a large-scale data lake, simply listing files can become a significant bottleneck. Hudi's Metadata Table is a powerful, self-managed Hudi table that tracks all file listings, partitions, and statistics, allowing for much faster queries and more efficient operations.\n",
    "\n",
    "In Hudi 1.x, these features have been further enhanced with the ability to manage indexes directly using SQL. This notebook will demonstrate:\n",
    "\n",
    "- ***What the Metadata Table is:*** We'll inspect the files that make up the Metadata Table.\n",
    "- ***The Performance Impact:*** We'll show how the Metadata Table speeds up file listing.\n",
    "- ***SQL-Based Index Management:*** We'll create, use, and drop indexes directly with SQL commands to optimize queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53e9c2-18d8-4d24-abbc-47b08cd98ffc",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "First, we begin by importing our necessary libraries and starting a SparkSession configured to work with Hudi and MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b507005d-b5a8-456a-bd5b-1b1f2f274e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab6d32-4503-49e9-ab2a-172a6f170b50",
   "metadata": {},
   "source": [
    "Now, let's start the SparkSession. We'll give it the app name 'HudiMetadataIndexing' and configure it to use our Hudi and MinIO settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9578a6df-9ba2-4956-ba1c-d282bf8041c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/26 12:45:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "spark = get_spark(\"HudiMetadataIndexing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7794052-d7e9-4de0-afbd-fad12b6f39e1",
   "metadata": {},
   "source": [
    "## Initial Table Creation\n",
    "We'll start with a simple dataset of ride data. This will be our main table, and we'll then explore its metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aedc4a7-8084-4420-b2ae-8b5f55ad5607",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .dataframe {\n",
       "            border-radius: 0.5rem;\n",
       "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\n",
       "            overflow-x: auto;\n",
       "            border: 1px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe th {\n",
       "            background-color: #f1f5f9;\n",
       "            color: #1f2937;\n",
       "            font-weight: 600;\n",
       "            padding: 0.75rem 1.5rem;\n",
       "            text-align: left;\n",
       "            border-bottom: 2px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe td {\n",
       "            padding: 0.75rem 1.5rem;\n",
       "            border-bottom: 1px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe tr:nth-child(even) {\n",
       "            background-color: #f8fafc;\n",
       "        }\n",
       "        .dataframe tr:hover {\n",
       "            background-color: #e2e8f0;\n",
       "            transition: background-color 0.2s ease-in-out;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe w-full border-collapse text-sm text-gray-900 dark:text-white\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ts</th>\n",
       "      <th>uuid</th>\n",
       "      <th>rider</th>\n",
       "      <th>driver</th>\n",
       "      <th>fare</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2025-08-10 08:15:30</td>\n",
       "      <td>uuid-001</td>\n",
       "      <td>rider-A</td>\n",
       "      <td>driver-X</td>\n",
       "      <td>18.50</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 09:22:10</td>\n",
       "      <td>uuid-002</td>\n",
       "      <td>rider-B</td>\n",
       "      <td>driver-Y</td>\n",
       "      <td>22.75</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 10:05:45</td>\n",
       "      <td>uuid-003</td>\n",
       "      <td>rider-C</td>\n",
       "      <td>driver-Z</td>\n",
       "      <td>14.60</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "initial_data = [\n",
    "    (\"2025-08-10 08:15:30\", \"uuid-001\", \"rider-A\", \"driver-X\", 18.50, \"new_york\"),\n",
    "    (\"2025-08-10 09:22:10\", \"uuid-002\", \"rider-B\", \"driver-Y\", 22.75, \"san_francisco\"),\n",
    "    (\"2025-08-10 10:05:45\", \"uuid-003\", \"rider-C\", \"driver-Z\", 14.60, \"chicago\")\n",
    "]\n",
    "initial_columns = [\"ts\", \"uuid\", \"rider\", \"driver\", \"fare\", \"city\"]\n",
    "initial_df = spark.createDataFrame(initial_data).toDF(*initial_columns)\n",
    "\n",
    "display(initial_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db4cdb-2180-419d-b4e0-066a15aac503",
   "metadata": {},
   "source": [
    "Now, let's create a Hudi table with a crucial configuration: ***\"hoodie.metadata.enable\": \"true\".*** This flag tells Hudi to maintain an internal Metadata Table, which will speed up our operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55b63196-53e9-4360-87e1-c649ed534f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "table_name = \"rides_metadata_table\"\n",
    "base_path = \"s3a://warehouse/hudi-metadata\"\n",
    "\n",
    "hudi_conf = {\n",
    "    \"hoodie.table.name\": table_name,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"uuid\",\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"city\",\n",
    "    \"hoodie.metadata.enable\": \"false\"\n",
    "}\n",
    "\n",
    "initial_df.write.format(\"hudi\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path}/{table_name}\")\n",
    "\n",
    "# Register a temp view to easily query the table\n",
    "#spark.read.format(\"hudi\").load(f\"{base_path}/{table_name}\").createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab201e8-fbe8-4651-8e16-4557bd8be88b",
   "metadata": {},
   "source": [
    "## The Hudi Metadata Table: A Deeper Look\n",
    "\n",
    "Hudi employs a special internal metadata table within each dataset to track metadata information - such as file listings and column statistics, helping avoid costly file system scans and improving read/write efficiency.\n",
    "\n",
    "***Key Features of the Metadata Table:***\n",
    "- ***Scalable:*** Capable of scaling to large sizes, handling TBs of metadata efficiently.\n",
    "- ***Flexible:*** Supports multi-modal indexing, allowing enabling/disabling various index types dynamically.\n",
    "- ***Fast Lookups:*** Uses an SSTable-like base file format (HFile) for fast partial scans and selective column reads.\n",
    "\n",
    "***The metadata table holds auxiliary data like:***\n",
    "- File indices for efficient record location\n",
    "- Column statistics for data skipping\n",
    "- Bloom filters for quick membership tests\n",
    "- Record and secondary indexes to speed up queries\n",
    "\n",
    "Let's look at the file system of our newly created table. Here three directories with city names, are partitions containing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af43b443-bd99-4298-b21d-e195f23a8808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/chicago\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/new_york\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/san_francisco\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c1b64e-0081-4c6e-b44f-41b9abd8125d",
   "metadata": {},
   "source": [
    "Now If you look inside a partition directory you will see following files. ***.hoodie_partition_metadata*** files store information about partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dc9e286-7920-42b7-bfc4-af4aa683fe7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-metadata/rides_metadata_table/new_york/.hoodie_partition_metadata\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/new_york/422dd207-ac0a-406c-a388-5211af047bec-0_1-16-85_20250826124514061.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name}/new_york\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b2e674-5dfe-408f-8a27-1d186f0b7043",
   "metadata": {},
   "source": [
    "The .hoodie directory contains subdirectories that store metadata files. Notice the special ***.hoodie/metadata*** directory. This is the Metadata Table itself. The files inside are not human-readable but are critical for Hudi's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26e27e9f-7122-4a87-a4a0-4f532a64a4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/hoodie.properties\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/.aux\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/.schema\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/.temp\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/timeline\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the Hudi table's .hoodie directory\n",
    "ls(f\"{base_path}/{table_name}/.hoodie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a4d25-af72-41c3-8000-5c780cca0342",
   "metadata": {},
   "source": [
    "The output shows several key directories and files:\n",
    "\n",
    "- ***.aux, .index_defs, .temp:*** These folders store internal metadata and temporary files.\n",
    "- ***.schema:*** This folder stores schema information for the Hudi table which helps in schema evolution.\n",
    "- ***timeline:*** This directory contains all the files that make up the Hudi Timeline, which is a record of every transaction that has occurred on the table.\n",
    "- ***metadata:*** This is the Metadata Table. It is itself a Hudi table and contains the file-level metadata like partition paths, file listings, and commit information that allows Hudi to quickly find files without performing a full file system scan.\n",
    "- ***hoodie.properties:*** The main configuration file for the table, which holds settings like the table name, key fields, and partitioning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58025316-b5fd-497d-bb8f-5b3350a6c765",
   "metadata": {},
   "source": [
    "Another crucial part of this metadata is the ***Hudi Timeline***, which consists of small files that log every change to the table. These meta-files follow the naming pattern below:\n",
    "\n",
    "[action timestamp].[action type].[action state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abab26eb-b37e-4e88-a8ae-69a2280e1c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/timeline/20250826124514061.commit.requested\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/timeline/20250826124514061.inflight\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/timeline/20250826124514061_20250826124516083.commit\n",
      "s3a://warehouse/hudi-metadata/rides_metadata_table/.hoodie/timeline/history\n"
     ]
    }
   ],
   "source": [
    "# List the contents of the Hudi table's timeline directory\n",
    "ls(f\"{base_path}/{table_name}/.hoodie/timeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58ff96-c416-4098-9a75-9265d4d2ae51",
   "metadata": {},
   "source": [
    "- An action timestamp is a unique, chronological identifier for each event, marking when it was scheduled.\n",
    "- An action type describes the operation that took place. Examples include commit or deltacommit for data changes, compaction or clean for maintenance, and savepoint or restore for recovery.\n",
    "- An action state shows the current status of the action. It can be requested (waiting to start), inflight (in progress), or commit (completed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a161b9be-90b1-495c-9a77-034b4a51506e",
   "metadata": {},
   "source": [
    "## Indexing Enhancements in Hudi 1.x\n",
    "Hudi 1.x introduces an advanced indexing subsystem that generalizes index capabilities closer to those found in relational databases.\n",
    "\n",
    "Important Enhancements:\n",
    "- ***Secondary Indexes:*** Support for indexes on any secondary columns to speed up query filtering.\n",
    "- ***Expression-Based Indexes:*** Indexes on expressions or transformed columns, enabling advanced data skipping.\n",
    "- ***SQL-Based Index Management:*** Users can create and manage indexes using standard SQL DDL commands via Spark SQL.\n",
    "- ***Asynchronous Indexing:*** Indexes can be built asynchronously alongside ongoing writes, improving write throughput without blocking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e713216-a482-4da8-bb33-b1f3d81d6113",
   "metadata": {},
   "source": [
    "## SQL-Based Index Creation and Management\n",
    "With Hudi 1.x, you can create different types of indexes directly on the Metadata Table using SQL. These indexes further accelerate query performance, especially for filtering on specific columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b743ab-ba57-411c-8682-5a6522d95ea5",
   "metadata": {},
   "source": [
    "### Example Commands:\n",
    "\n",
    "- Enable record index (dependency for secondary index)\n",
    "=> SET hoodie.metadata.record.index.enable=true;\n",
    "\n",
    "- Create record index on primary key column (e.g., uuid)\n",
    "=> CREATE INDEX record_index ON hudi_table (uuid);\n",
    "\n",
    "- Create secondary index on 'rider' column\n",
    "=> CREATE INDEX idx_rider ON hudi_table (rider);\n",
    "\n",
    "- Create bloom filter index on 'driver' column\n",
    "=> CREATE INDEX idx_bloom_driver ON hudi_table USING bloom_filters(driver) OPTIONS(expr='identity');\n",
    "\n",
    "- Create expression-based column stats index on timestamp column\n",
    "=> CREATE INDEX idx_column_ts ON hudi_table USING column_stats(ts) OPTIONS(expr='from_unixtime', format='yyyy-MM-dd');\n",
    "\n",
    "- Drop indexes when no longer needed\n",
    "=> DROP INDEX record_index ON hudi_table;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a24e1-1d1b-4dbb-ac7f-3052abe39cab",
   "metadata": {},
   "source": [
    "### Practical Example Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ace4895-b7b0-4037-8f8c-68b1514f542e",
   "metadata": {},
   "source": [
    "***1. Create a Spark SQL Table on Hudi Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a0e1a0-fa2c-42a8-b260-12e4a8d345b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE {table_name} (\n",
    "    ts BIGINT,\n",
    "    uuid STRING,\n",
    "    rider STRING,\n",
    "    driver STRING,\n",
    "    fare DOUBLE,\n",
    "    city STRING\n",
    " ) USING hudi \n",
    "     options(\n",
    "        primaryKey ='uuid'\n",
    ")\n",
    "PARTITIONED BY (city)\n",
    "LOCATION '{base_path}';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eced8062-a574-4798-b8d1-e971e5a88461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: string, value: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Enable record index (required for secondary index)\n",
    "spark.sql(\"SET hoodie.metadata.record.index.enable=false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ca3b00b-1b51-4886-944a-30036df8b539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o49.sql.\n: org.apache.hudi.exception.HoodieException: Metadata table is not yet initialized. Initialize FILES partition before any other partition [Metadata partition {name: record_index, prefix: record-index-}]\n\tat org.apache.hudi.index.HoodieSparkIndexClient.doSchedule(HoodieSparkIndexClient.java:234)\n\tat org.apache.hudi.index.HoodieSparkIndexClient.createRecordIndex(HoodieSparkIndexClient.java:117)\n\tat org.apache.hudi.index.HoodieSparkIndexClient.create(HoodieSparkIndexClient.java:100)\n\tat org.apache.spark.sql.hudi.command.CreateIndexCommand.run(IndexCommands.scala:69)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Step 2: Create Record Index on primary key 'uuid'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCREATE INDEX record_index ON \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtable_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m (uuid)\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o49.sql.\n: org.apache.hudi.exception.HoodieException: Metadata table is not yet initialized. Initialize FILES partition before any other partition [Metadata partition {name: record_index, prefix: record-index-}]\n\tat org.apache.hudi.index.HoodieSparkIndexClient.doSchedule(HoodieSparkIndexClient.java:234)\n\tat org.apache.hudi.index.HoodieSparkIndexClient.createRecordIndex(HoodieSparkIndexClient.java:117)\n\tat org.apache.hudi.index.HoodieSparkIndexClient.create(HoodieSparkIndexClient.java:100)\n\tat org.apache.spark.sql.hudi.command.CreateIndexCommand.run(IndexCommands.scala:69)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Create Record Index on primary key 'uuid'\n",
    "spark.sql(f\"CREATE INDEX record_index ON {table_name} (uuid)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b917c-f363-4310-a865-ae6ed7effb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create Secondary Index on 'rider' column\n",
    "spark.sql(f\"CREATE INDEX idx_rider ON {table_name} (rider)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43538a8a-d129-4fef-a1ac-150018d1b003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b81e7-c2a0-4b99-9635-b324f1810591",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
