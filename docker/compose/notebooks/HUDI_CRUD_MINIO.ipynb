{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e45bef4-757a-4814-a20e-04d911d77707",
   "metadata": {},
   "source": [
    "## Getting Started with Hudi: A Hands-on Guide to CRUD Operations\n",
    "\n",
    "This notebook is a practical guide to performing CRUD (Create, Read, Update, Delete) operations on an Apache Hudi table using PySpark. We'll be using MinIO as our S3-compatible storage backend, demonstrating how to handle modern data lake architecture. We will work with both Copy-On-Write (COW) and Merge-On-Read (MOR) tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e1a108-3626-4767-aa69-a114303e28e9",
   "metadata": {},
   "source": [
    "### Setting up the Spark Environment\n",
    "\n",
    "We begin by loading the init_spark.ipynb notebook, which contains the necessary imports and functions to start a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2475e0ef-c9eb-482b-bd4f-5bc1fb48312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/init_spark.ipynb\n",
    "%run utils/s3_utils.ipynb\n",
    "%run utils/s3_utils_updated.ipynb\n",
    "%run utils/display_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889e65e-2528-46fa-8970-e070d4a660d5",
   "metadata": {},
   "source": [
    "Now, let's start the SparkSession. We'll give it the app name 'Hudi-Jupyter' and configure it to use our Hudi and MinIO settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "839c114f-8ed4-4d56-828c-d2f81cf959b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "spark = get_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a5a5c-af44-4aba-9666-9791b2540e45",
   "metadata": {},
   "source": [
    "This is our initial dataset for our Hudi table. It's a list of ride records with columns for timestamp, a unique ID, rider and driver names, the fare, and the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c7c84b1-fc73-47c5-a944-36f7a5a23149",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"ts\", \"uuid\", \"rider\", \"driver\", \"fare\", \"city\"]\n",
    "data = [\n",
    "    (\"2025-08-10 08:15:30\", \"uuid-001\", \"rider-A\", \"driver-X\", 18.50, \"new_york\"),\n",
    "    (\"2025-08-10 09:22:10\", \"uuid-002\", \"rider-B\", \"driver-Y\", 22.75, \"san_francisco\"),\n",
    "    (\"2025-08-10 10:05:45\", \"uuid-003\", \"rider-C\", \"driver-Z\", 14.60, \"chicago\"),\n",
    "    (\"2025-08-10 11:40:00\", \"uuid-004\", \"rider-D\", \"driver-W\", 31.90, \"new_york\"),\n",
    "    (\"2025-08-10 12:55:15\", \"uuid-005\", \"rider-E\", \"driver-V\", 25.10, \"san_francisco\"),\n",
    "    (\"2025-08-10 13:20:35\", \"uuid-006\", \"rider-F\", \"driver-U\", 19.80, \"chicago\"),\n",
    "    (\"2025-08-10 14:10:05\", \"uuid-007\", \"rider-G\", \"driver-T\", 28.45, \"san_francisco\"),\n",
    "    (\"2025-08-10 15:00:20\", \"uuid-008\", \"rider-H\", \"driver-S\", 16.25, \"new_york\"),\n",
    "    (\"2025-08-10 15:45:50\", \"uuid-009\", \"rider-I\", \"driver-R\", 24.35, \"chicago\"),\n",
    "    (\"2025-08-10 16:30:00\", \"uuid-010\", \"rider-J\", \"driver-Q\", 20.00, \"new_york\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db4d857-903f-4235-9539-5451d6ac30ad",
   "metadata": {},
   "source": [
    "First, we are creating a PySpark DataFrame from our sample data. Let's take a quick look at the data to see what we're starting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1da470c-62da-44c6-9228-05b45a14e9ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .dataframe {\n",
       "            border-radius: 0.5rem;\n",
       "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\n",
       "            overflow-x: auto;\n",
       "            border: 1px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe th {\n",
       "            background-color: #f1f5f9;\n",
       "            color: #1f2937;\n",
       "            font-weight: 600;\n",
       "            padding: 0.75rem 1.5rem;\n",
       "            text-align: left;\n",
       "            border-bottom: 2px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe td {\n",
       "            padding: 0.75rem 1.5rem;\n",
       "            border-bottom: 1px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe tr:nth-child(even) {\n",
       "            background-color: #f8fafc;\n",
       "        }\n",
       "        .dataframe tr:hover {\n",
       "            background-color: #e2e8f0;\n",
       "            transition: background-color 0.2s ease-in-out;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe w-full border-collapse text-sm text-gray-900 dark:text-white\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ts</th>\n",
       "      <th>uuid</th>\n",
       "      <th>rider</th>\n",
       "      <th>driver</th>\n",
       "      <th>fare</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2025-08-10 08:15:30</td>\n",
       "      <td>uuid-001</td>\n",
       "      <td>rider-A</td>\n",
       "      <td>driver-X</td>\n",
       "      <td>18.50</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 09:22:10</td>\n",
       "      <td>uuid-002</td>\n",
       "      <td>rider-B</td>\n",
       "      <td>driver-Y</td>\n",
       "      <td>22.75</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 10:05:45</td>\n",
       "      <td>uuid-003</td>\n",
       "      <td>rider-C</td>\n",
       "      <td>driver-Z</td>\n",
       "      <td>14.60</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 11:40:00</td>\n",
       "      <td>uuid-004</td>\n",
       "      <td>rider-D</td>\n",
       "      <td>driver-W</td>\n",
       "      <td>31.90</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 12:55:15</td>\n",
       "      <td>uuid-005</td>\n",
       "      <td>rider-E</td>\n",
       "      <td>driver-V</td>\n",
       "      <td>25.10</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 13:20:35</td>\n",
       "      <td>uuid-006</td>\n",
       "      <td>rider-F</td>\n",
       "      <td>driver-U</td>\n",
       "      <td>19.80</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 14:10:05</td>\n",
       "      <td>uuid-007</td>\n",
       "      <td>rider-G</td>\n",
       "      <td>driver-T</td>\n",
       "      <td>28.45</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 15:00:20</td>\n",
       "      <td>uuid-008</td>\n",
       "      <td>rider-H</td>\n",
       "      <td>driver-S</td>\n",
       "      <td>16.25</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 15:45:50</td>\n",
       "      <td>uuid-009</td>\n",
       "      <td>rider-I</td>\n",
       "      <td>driver-R</td>\n",
       "      <td>24.35</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 16:30:00</td>\n",
       "      <td>uuid-010</td>\n",
       "      <td>rider-J</td>\n",
       "      <td>driver-Q</td>\n",
       "      <td>20.00</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "inputDF = spark.createDataFrame(data).toDF(*columns)\n",
    "#inputDF.show(truncate = False)\n",
    "display(inputDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e5207-9d46-4a6b-8265-aa5b0384a4e7",
   "metadata": {},
   "source": [
    "## Copy-on-Write (COW) Tables\n",
    "\n",
    "We will now explore the Copy-On-Write (COW) storage type. In a COW table, each time data is updated or deleted in a file, Hudi rewrites the entire file with the new data. This is a simpler and more traditional approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b173b-794e-400b-b5a9-309ee8cf2fb9",
   "metadata": {},
   "source": [
    "### HUDI Configuration\n",
    "\n",
    "Next, we will set up the specific configuration for our Hudi table. We'll use uuid as our unique record key and partition the data by city to keep it organized. The ts (timestamp) field is our precombine key, which helps Hudi decide which record to keep if it finds duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb2d37f-f1d5-4105-b690-815fdf29cb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_cow = \"trips_table_cow\"\n",
    "base_path = f\"s3a://warehouse/hudi-db\"\n",
    "\n",
    "hudi_conf = {\n",
    "    \"hoodie.table.name\": table_name_cow,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"uuid\",\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"city\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.write.markers.type\": \"DIRECT\",\n",
    "    \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c4df7c-7ae7-42a5-902e-8bfebc0c63b4",
   "metadata": {},
   "source": [
    "### Inserting data\n",
    "\n",
    "This is the **\"Create\"** part of our CRUD operations. We are writing our initial DataFrame to MinIO as a Hudi table. Using mode(\"overwrite\") ensures that we start with a fresh table every time we run to ensure a clean start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "037b6350-c696-4ddc-8c27-19b1f041af0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# WARNING: Unable to attach Serviceability Agent. Unable to attach even with module exceptions: [org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed., org.apache.hudi.org.openjdk.jol.vm.sa.SASupportException: Sense failed.]\n"
     ]
    }
   ],
   "source": [
    "# Write the DataFrame to a Hudi COW table\n",
    "inputDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"insert\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path}/{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6bbc9-c480-4bae-97d6-f8aa6d84cc0c",
   "metadata": {},
   "source": [
    "There are two main types of files:\n",
    "\n",
    "- Metadata files located in <base_path>/.hoodie/\n",
    "- Data files stored within partition paths for partitioned tables, or under the base path for non-partitioned tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1978002-cb13-4e6d-912c-db8c6681ddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=chicago\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=new_york\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54887ab0-4579-4deb-bf05-9583f4596683",
   "metadata": {},
   "source": [
    "Hudi manages a table's metadata by storing it in a special directory within the base path. This metadata helps ensure that all tools reading and writing to the table follow the same rules.\n",
    "One of the key files is hoodie.properties, which acts like a configuration file for the table, holding details like table name and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b43ca01e-e3d1-4485-92a8-c8f22b743052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/hoodie.properties\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/.aux\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/.index_defs\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/.schema\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/.temp\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/metadata\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/timeline\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name_cow}/.hoodie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556c8241-eb07-4c24-8e20-c97223a4ba30",
   "metadata": {},
   "source": [
    "Another crucial part of this metadata is the Hudi Timeline, which consists of small files that log every change to the table. These meta-files follow the naming pattern below:\n",
    "\n",
    "[action timestamp].[action type].[action state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17c497a-4fe8-4933-88f5-bdb5fa7ac7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/timeline/20250813173932729.commit.requested\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/timeline/20250813173932729.inflight\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/timeline/20250813173932729_20250813173936142.commit\n",
      "s3a://warehouse/hudi-db/trips_table_cow/.hoodie/timeline/history\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name_cow}/.hoodie/timeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd821b-c3d8-4554-bc25-b8e98c0bec73",
   "metadata": {},
   "source": [
    "- An action timestamp is a unique, chronological identifier for each event, marking when it was scheduled.\n",
    "- An action type describes the operation that took place. Examples include commit or deltacommit for data changes, compaction or clean for maintenance, and savepoint or restore for recovery.\n",
    "- An action state shows the current status of the action. It can be **requested** (waiting to start), **inflight** (in progress), or **commit** (completed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9ff4d-7a22-4fc8-90c9-e3873f907824",
   "metadata": {},
   "source": [
    "Now coming to the data management in Hudi, Hudi categorizes physical data files into Base File and Log File:\n",
    "\n",
    "- Base File contains the main stored records in a Hudi table and is optimized for read.\n",
    "- Log File contains the records' changes on top of its associated Base File and is optimized for write.\n",
    "\n",
    "Within a partition path of a Hudi table, a single Base File and its associated Log Files (in case of MOR table) are grouped together as a File Slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3457612-7f25-409e-97e8-1d907007a14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.hoodie_partition_metadata\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-24-56_20250813173932729.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name_cow}/city=san_francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42c3dfd-bd57-4cf2-9b2e-db988b1dc81e",
   "metadata": {},
   "source": [
    "To easily query our newly created Hudi table, we first need to register it as a temporary SQL view. After that, we'll run a quick command to list all tables and confirm that it's ready to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00dbecfc-731e-42c7-a7e4-c76e05946cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"hudi\").load(f\"{base_path}/{table_name_cow}\").createOrReplaceTempView(f\"{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8c85b1-fe00-413a-8ede-1385a986b827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------+\n",
      "|namespace|tableName      |isTemporary|\n",
      "+---------+---------------+-----------+\n",
      "|         |trips_table_cow|false      |\n",
      "+---------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9ee89c-8aa7-44ba-a77e-38c23e9b2625",
   "metadata": {},
   "source": [
    "### Upserting Records (Update)\n",
    "\n",
    "Hudi's upsert is a powerful feature that allows us to insert new records or update existing ones. We are going to update the fare for 'rider-G' by multiplying it by 10. Here, we are showing the updated record before we apply the change to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68067ecd-d30c-4195-bc42-cf3f8b4bda18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "|    uuid|  rider|  driver| fare|         city|                 ts|\n",
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "|uuid-007|rider-G|driver-T|284.5|san_francisco|2025-08-10 14:10:05|\n",
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "updatesDF = spark.read.format(\"hudi\").load(f\"{base_path}/{table_name_cow}\").filter(col(\"rider\") == \"rider-G\").withColumn(\"fare\", col(\"fare\") * 10)\n",
    "\n",
    "updatesDF.select(\"uuid\", \"rider\", \"driver\", \"fare\", \"city\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ff3eb4-3f17-4a8e-9e64-adb2f711fa8a",
   "metadata": {},
   "source": [
    "### Upserting the modified record\n",
    "Now, we'll perform the upsert. Because uuid is our record key, Hudi knows to find the original record and replace it with our new, updated one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54f5c72-e6b2-4f93-9884-ceac656b5d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatesDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb00031-4f2a-4012-98ef-3235c6480337",
   "metadata": {},
   "source": [
    "Let's check the files in the **san_francisco** partition to see what happened after the upsert. Since this is a Copy-on-Write table, Hudi didn't just modify the existing record in place. Instead, it created a brand new Parquet file containing the updated record and all the other records for that partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e2db32a-2851-46c4-ae67-a353609bc566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.24ca20d9-ed19-4507-a3f7-f31e74956271-0_20250813173937705.log.1_0-68-174.cdc\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.hoodie_partition_metadata\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-24-56_20250813173932729.parquet\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-68-174_20250813173937705.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(f\"{base_path}/{table_name_cow}/city=san_francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa8f80-a490-47c8-a302-f538144943e7",
   "metadata": {},
   "source": [
    "### Snapshot Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84315bfa-0b58-40df-b0ee-7e8f85b308d5",
   "metadata": {},
   "source": [
    "This is the default query type when reading Hudi tables. Its goal is to give you a complete, up-to-the-minute view of your data. When you run this query on a Merge-on-Read (MOR) table, Hudi merges the recent changes from the log files with the base data files to present the latest records, which can affect performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1347b251-84d7-4628-bfb5-be285217e9a3",
   "metadata": {},
   "source": [
    "Let's do a quick snapshot query to see the current state of our table. Notice the new _hoodie_commit_time for the updated record, which shows when the change was made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9dc3e00-8903-4cc1-9ad3-813bdf803f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "|_hoodie_commit_time|    uuid|  rider|  driver| fare|         city|                 ts|\n",
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "|  20250813173932729|uuid-002|rider-B|driver-Y|22.75|san_francisco|2025-08-10 09:22:10|\n",
      "|  20250813173932729|uuid-005|rider-E|driver-V| 25.1|san_francisco|2025-08-10 12:55:15|\n",
      "|  20250813173937705|uuid-007|rider-G|driver-T|284.5|san_francisco|2025-08-10 14:10:05|\n",
      "|  20250813173932729|uuid-001|rider-A|driver-X| 18.5|     new_york|2025-08-10 08:15:30|\n",
      "|  20250813173932729|uuid-004|rider-D|driver-W| 31.9|     new_york|2025-08-10 11:40:00|\n",
      "|  20250813173932729|uuid-008|rider-H|driver-S|16.25|     new_york|2025-08-10 15:00:20|\n",
      "|  20250813173932729|uuid-010|rider-J|driver-Q| 20.0|     new_york|2025-08-10 16:30:00|\n",
      "|  20250813173932729|uuid-003|rider-C|driver-Z| 14.6|      chicago|2025-08-10 10:05:45|\n",
      "|  20250813173932729|uuid-006|rider-F|driver-U| 19.8|      chicago|2025-08-10 13:20:35|\n",
      "|  20250813173932729|uuid-009|rider-I|driver-R|24.35|      chicago|2025-08-10 15:45:50|\n",
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshotQueryDF = spark.read \\\n",
    "        .format(\"hudi\") \\\n",
    "        .load(f\"{base_path}/{table_name_cow}\" + \"/*/*\")\n",
    "\n",
    "snapshotQueryDF.select(\"_hoodie_commit_time\", \"uuid\", \"rider\", \"driver\", \"fare\", \"city\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6788562-94bf-493e-ade8-4cd0d621f618",
   "metadata": {},
   "source": [
    "After refreshing our table's metadata, we can run a SQL query to see the updated fare for 'rider-G'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a338a22b-9729-4296-ad37-968abbc9958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "|uuid    |rider  |driver  |fare |city         |ts                 |\n",
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "|uuid-002|rider-B|driver-Y|22.75|san_francisco|2025-08-10 09:22:10|\n",
      "|uuid-005|rider-E|driver-V|25.1 |san_francisco|2025-08-10 12:55:15|\n",
      "|uuid-007|rider-G|driver-T|284.5|san_francisco|2025-08-10 14:10:05|\n",
      "|uuid-001|rider-A|driver-X|18.5 |new_york     |2025-08-10 08:15:30|\n",
      "|uuid-004|rider-D|driver-W|31.9 |new_york     |2025-08-10 11:40:00|\n",
      "|uuid-008|rider-H|driver-S|16.25|new_york     |2025-08-10 15:00:20|\n",
      "|uuid-010|rider-J|driver-Q|20.0 |new_york     |2025-08-10 16:30:00|\n",
      "|uuid-003|rider-C|driver-Z|14.6 |chicago      |2025-08-10 10:05:45|\n",
      "|uuid-006|rider-F|driver-U|19.8 |chicago      |2025-08-10 13:20:35|\n",
      "|uuid-009|rider-I|driver-R|24.35|chicago      |2025-08-10 15:45:50|\n",
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"REFRESH TABLE {table_name_cow}\")\n",
    "tripsDF_read = spark.sql(f\"select uuid, rider, driver, fare, city, ts from {table_name_cow}\")\n",
    "tripsDF_read.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c22a4b-c9b5-49e6-b97a-d0648d9218b9",
   "metadata": {},
   "source": [
    "### Incremental Reads\n",
    "\n",
    "Hudi's incremental query feature lets us efficiently process only the data that has changed since a specific point in time. We'll start by listing all the commit times in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2db9a5c7-be14-43b0-a692-d2036b6616f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       commitTime|\n",
      "+-----------------+\n",
      "|20250813173932729|\n",
      "|20250813173937705|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"select distinct(_hoodie_commit_time) as commitTime from {table_name_cow} order by commitTime\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c2631-cad2-49bc-b990-dd8249d43568",
   "metadata": {},
   "source": [
    "Now, let's configure an incremental read to grab only the data committed after our initial write operation. Let's fetch the latest commit from the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1713699-a897-4da9-8b3f-5e28ab4b1d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incremental commit time: 20250813173937705\n"
     ]
    }
   ],
   "source": [
    "# Get distinct commit times ordered\n",
    "commits_df = spark.sql(f\"SELECT DISTINCT(_hoodie_commit_time) AS commitTime FROM {table_name_cow} ORDER BY commitTime\")\n",
    "\n",
    "# Collect top 50 commit times as a list\n",
    "commits = [row['commitTime'] for row in commits_df.take(50)]\n",
    "\n",
    "incrementalTime = commits[-1]  # Commit time we are interested in\n",
    "print(f\"Incremental commit time: {incrementalTime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04b87d74-1803-48f2-88ad-36fb9490f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "incremental_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': incrementalTime,\n",
    "}\n",
    "\n",
    "incrementalQueryDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**incremental_read_options). \\\n",
    "  load(f\"{base_path}/{table_name_cow}\")\n",
    "incrementalQueryDF.createOrReplaceTempView(\"trips_incremental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b36db80-4e11-41ef-8514-8d5840fe1ba5",
   "metadata": {},
   "source": [
    "When we query our temporary incremental table, you can see that it returns only the single record that was updated since our last write operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79e3ce25-c675-4882-b0cc-ff76f4735ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "|_hoodie_commit_time|uuid    |rider  |driver  |fare |city         |ts                 |\n",
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "|20250813173937705  |uuid-007|rider-G|driver-T|284.5|san_francisco|2025-08-10 14:10:05|\n",
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select _hoodie_commit_time, uuid, rider, driver, fare, city, ts from trips_incremental\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d5bd12e-3466-48ef-9c81-d05cc595d535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin commit time: 20250813173932729\n"
     ]
    }
   ],
   "source": [
    "# Get distinct commit times ordered\n",
    "commits_df = spark.sql(f\"SELECT DISTINCT(_hoodie_commit_time) AS commitTime FROM {table_name_cow} ORDER BY commitTime\")\n",
    "\n",
    "# Collect top 50 commit times as a list\n",
    "commits = [row['commitTime'] for row in commits_df.take(50)]\n",
    "\n",
    "beginTime = commits[-2]  # Commit time we are interested in\n",
    "print(f\"Begin commit time: {beginTime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8b496-42be-462f-bc4a-f0ebfc88b1e0",
   "metadata": {},
   "source": [
    "### Time Travel Query\n",
    "Hudi also allows for time travel, which means we can query the state of our table at a specific point in the past. By specifying the commit time from our initial data insertion, we can view the table's contents before we performed the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bb6151f-5a14-4aa1-abdb-44c52c7f2d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"hudi\") \\\n",
    "  .option(\"as.of.instant\", beginTime) \\\n",
    "  .load(f\"{base_path}/{table_name_cow}\").createOrReplaceTempView(\"trips_time_travel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a5a03c-e406-4149-b9a4-748b5f876aab",
   "metadata": {},
   "source": [
    "As you can see, querying the historical view shows the original fare for 'rider-G' before we updated it. This is a great way to audit or restore data from the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06e38a37-9cff-44f5-af44-c30176422f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "|_hoodie_commit_time|uuid    |rider  |driver  |fare |city         |ts                 |\n",
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "|20250813173932729  |uuid-002|rider-B|driver-Y|22.75|san_francisco|2025-08-10 09:22:10|\n",
      "|20250813173932729  |uuid-005|rider-E|driver-V|25.1 |san_francisco|2025-08-10 12:55:15|\n",
      "|20250813173932729  |uuid-007|rider-G|driver-T|28.45|san_francisco|2025-08-10 14:10:05|\n",
      "|20250813173932729  |uuid-001|rider-A|driver-X|18.5 |new_york     |2025-08-10 08:15:30|\n",
      "|20250813173932729  |uuid-004|rider-D|driver-W|31.9 |new_york     |2025-08-10 11:40:00|\n",
      "|20250813173932729  |uuid-008|rider-H|driver-S|16.25|new_york     |2025-08-10 15:00:20|\n",
      "|20250813173932729  |uuid-010|rider-J|driver-Q|20.0 |new_york     |2025-08-10 16:30:00|\n",
      "|20250813173932729  |uuid-003|rider-C|driver-Z|14.6 |chicago      |2025-08-10 10:05:45|\n",
      "|20250813173932729  |uuid-006|rider-F|driver-U|19.8 |chicago      |2025-08-10 13:20:35|\n",
      "|20250813173932729  |uuid-009|rider-I|driver-R|24.35|chicago      |2025-08-10 15:45:50|\n",
      "+-------------------+--------+-------+--------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select _hoodie_commit_time, uuid, rider, driver, fare, city, ts from trips_time_travel\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a9835b-c4e5-4e6e-b684-27a6c8711c39",
   "metadata": {},
   "source": [
    "## Change Data Capture (CDC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481ef03-69fd-4bd6-aa67-7a8917323344",
   "metadata": {},
   "source": [
    "Hudi's Change Data Capture (CDC) feature lets you read a stream of all the changes (inserts, updates, and deletes) that have been applied to your table. This is perfect for downstream systems that need to react to data modifications in real-time. We'll start by adding some new data and updating an existing record to generate some changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70e54a50-98c8-4158-ae8c-bca0ab8a3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Define a DataFrame with one new record and one updated record\n",
    "cdc_data = [\n",
    "    (\"2025-08-11 10:00:00\", \"uuid-011\", \"rider-K\", \"driver-P\", 10.50, \"chicago\"), # new record\n",
    "    (\"2025-08-10 09:22:10\", \"uuid-002\", \"rider-B\", \"driver-Y\", 50.00, \"san_francisco\") # updated record\n",
    "]\n",
    "\n",
    "cdc_columns = [\"ts\", \"uuid\", \"rider\", \"driver\", \"fare\", \"city\"]\n",
    "cdcDF = spark.createDataFrame(cdc_data).toDF(*cdc_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6319efd-1ec9-47bc-8b13-4c39fe0480ae",
   "metadata": {},
   "source": [
    "Now, we'll perform an upsert with our new data. This will create a new commit with one insert and one update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7dba895-6c71-4df6-b555-430fa047ec1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdcDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbfddcd-e700-4b33-92e3-47213a45fcd5",
   "metadata": {},
   "source": [
    "To see the changes from this specific transaction, we'll first get its commit time. We'll then use this as our starting point for the CDC query to capture all the changes from that moment forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9a239e7-570d-46cc-b532-4396cf176f31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'table_name_cow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m latest_commit_time \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect max(_hoodie_commit_time) from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name_cow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLatest commit time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_commit_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'table_name_cow' is not defined"
     ]
    }
   ],
   "source": [
    "latest_commit_time = spark.sql(f\"select max(_hoodie_commit_time) from {table_name_cow}\").collect()[0][0]\n",
    "print(f\"Latest commit time: {latest_commit_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6364ca-2fa3-42f0-a6b0-b50627036f37",
   "metadata": {},
   "source": [
    "Now we can perform a CDC query using a special incremental format. We'll set the query type to \"incremental\" and specify \"hoodie.datasource.query.incremental.format\": \"cdc\". By using the latest_commit_time we just fetched, we can capture all the changes from our last commit. The output will include the op column, which tells us whether a record was inserted, updated, or deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab932e17-4fe5-4bca-8663-8eecb64cd409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|op |ts_ms            |before                                                                                                                             |after                                                                                                                             |\n",
      "+---+-----------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|u  |20250813173940238|{\"ts\": \"2025-08-10 09:22:10\", \"uuid\": \"uuid-002\", \"rider\": \"rider-B\", \"driver\": \"driver-Y\", \"fare\": 22.75, \"city\": \"san_francisco\"}|{\"ts\": \"2025-08-10 09:22:10\", \"uuid\": \"uuid-002\", \"rider\": \"rider-B\", \"driver\": \"driver-Y\", \"fare\": 50.0, \"city\": \"san_francisco\"}|\n",
      "|i  |20250813173940238|null                                                                                                                               |{\"ts\": \"2025-08-11 10:00:00\", \"uuid\": \"uuid-011\", \"rider\": \"rider-K\", \"driver\": \"driver-P\", \"fare\": 10.5, \"city\": \"chicago\"}      |\n",
      "+---+-----------------+-----------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cdc_read_options = {\n",
    "  'hoodie.datasource.query.type': 'incremental',\n",
    "  'hoodie.datasource.read.begin.instanttime': latest_commit_time,\n",
    "  'hoodie.datasource.query.incremental.format': 'cdc'\n",
    "}\n",
    "\n",
    "cdcQueryDF = spark.read.format(\"hudi\"). \\\n",
    "  options(**cdc_read_options). \\\n",
    "  load(f\"{base_path}/{table_name_cow}\").show(truncate=False)\n",
    "#cdcQueryDF.select(\"_hoodie_commit_time\", \"_hoodie_operation\", \"uuid\", \"rider\", \"fare\", \"city\", \"ts\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913f6df-60d4-4be0-ad4c-beaa5beb32e9",
   "metadata": {},
   "source": [
    "Let's look at the above output to see what happened:\n",
    "\n",
    "**Update:** We have a record where **op is u**. This corresponds to the update we made to uuid-002. The before column shows the original fare of 22.75, and the after column shows the new fare of 50.0.\n",
    "\n",
    "**Insert:** We also have a record where **op is i**. This is the new record for uuid-011. The before column is null because it didn't exist before this commit, while the after column contains all the new record's data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176719d-533d-432b-b2bf-54e8343f3d4f",
   "metadata": {},
   "source": [
    "### Delete a record\n",
    "Finally, let's demonstrate how to delete a record. We'll remove the record for 'rider-G' from our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6d00e2d-37c3-4265-9bbb-8f9081186c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatesDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"delete\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b99617-e638-4161-bae7-143d6a505dbc",
   "metadata": {},
   "source": [
    "Since this is a Copy-on-Write table, Hudi will not simply remove the record in place. Instead, it will rewrite the Parquet file in the city=san_francisco partition, creating a new file that contains all the original records for that partition except for the one we deleted. We can confirm this by checking the file system and seeing a new Parquet file with a fresh commit timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d10795b0-46d1-4133-af01-c74359ede26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.24ca20d9-ed19-4507-a3f7-f31e74956271-0_20250813173937705.log.1_0-68-174.cdc\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.24ca20d9-ed19-4507-a3f7-f31e74956271-0_20250813173940238.log.1_0-117-305.cdc\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.24ca20d9-ed19-4507-a3f7-f31e74956271-0_20250813173941637.log.1_0-161-388.cdc\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/.hoodie_partition_metadata\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-117-305_20250813173940238.parquet\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-161-388_20250813173941637.parquet\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-24-56_20250813173932729.parquet\n",
      "s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco/24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-68-174_20250813173937705.parquet\n"
     ]
    }
   ],
   "source": [
    "ls(\"s3a://warehouse/hudi-db/trips_table_cow/city=san_francisco\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5351d820-ace0-4c40-919d-d44e674b6dad",
   "metadata": {},
   "source": [
    "Running a snapshot query now confirms that the record for **'rider-G' (uuid-007)** is no longer present in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f179e26-dc1d-4240-9993-a0db281eafb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------------+------------------+----------------------+--------------------------------------------------------------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                         |ts                 |uuid    |rider  |driver  |fare |city         |\n",
      "+-------------------+---------------------+------------------+----------------------+--------------------------------------------------------------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|20250813173932729  |20250813173932729_2_0|uuid-001          |city=new_york         |e1096794-e164-4c38-81db-737ab3c3738f-0_2-24-58_20250813173932729.parquet  |2025-08-10 08:15:30|uuid-001|rider-A|driver-X|18.5 |new_york     |\n",
      "|20250813173932729  |20250813173932729_2_1|uuid-004          |city=new_york         |e1096794-e164-4c38-81db-737ab3c3738f-0_2-24-58_20250813173932729.parquet  |2025-08-10 11:40:00|uuid-004|rider-D|driver-W|31.9 |new_york     |\n",
      "|20250813173932729  |20250813173932729_2_2|uuid-008          |city=new_york         |e1096794-e164-4c38-81db-737ab3c3738f-0_2-24-58_20250813173932729.parquet  |2025-08-10 15:00:20|uuid-008|rider-H|driver-S|16.25|new_york     |\n",
      "|20250813173932729  |20250813173932729_2_3|uuid-010          |city=new_york         |e1096794-e164-4c38-81db-737ab3c3738f-0_2-24-58_20250813173932729.parquet  |2025-08-10 16:30:00|uuid-010|rider-J|driver-Q|20.0 |new_york     |\n",
      "|20250813173932729  |20250813173932729_1_0|uuid-003          |city=chicago          |30976af4-d5b7-44ad-8eec-147340c3ed56-0_1-117-306_20250813173940238.parquet|2025-08-10 10:05:45|uuid-003|rider-C|driver-Z|14.6 |chicago      |\n",
      "|20250813173932729  |20250813173932729_1_1|uuid-006          |city=chicago          |30976af4-d5b7-44ad-8eec-147340c3ed56-0_1-117-306_20250813173940238.parquet|2025-08-10 13:20:35|uuid-006|rider-F|driver-U|19.8 |chicago      |\n",
      "|20250813173932729  |20250813173932729_1_2|uuid-009          |city=chicago          |30976af4-d5b7-44ad-8eec-147340c3ed56-0_1-117-306_20250813173940238.parquet|2025-08-10 15:45:50|uuid-009|rider-I|driver-R|24.35|chicago      |\n",
      "|20250813173940238  |20250813173940238_1_3|uuid-011          |city=chicago          |30976af4-d5b7-44ad-8eec-147340c3ed56-0_1-117-306_20250813173940238.parquet|2025-08-11 10:00:00|uuid-011|rider-K|driver-P|10.5 |chicago      |\n",
      "|20250813173940238  |20250813173940238_0_0|uuid-002          |city=san_francisco    |24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-161-388_20250813173941637.parquet|2025-08-10 09:22:10|uuid-002|rider-B|driver-Y|50.0 |san_francisco|\n",
      "|20250813173932729  |20250813173932729_0_1|uuid-005          |city=san_francisco    |24ca20d9-ed19-4507-a3f7-f31e74956271-0_0-161-388_20250813173941637.parquet|2025-08-10 12:55:15|uuid-005|rider-E|driver-V|25.1 |san_francisco|\n",
      "+-------------------+---------------------+------------------+----------------------+--------------------------------------------------------------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshotQueryDF = spark.read \\\n",
    "        .format(\"hudi\") \\\n",
    "        .load(f\"{base_path}/{table_name_cow}\" + \"/*/*\")\n",
    "\n",
    "snapshotQueryDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f3b34fbc-7566-410f-955d-fec201c7b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "|uuid    |rider  |driver  |fare |city         |ts                 |\n",
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "|uuid-001|rider-A|driver-X|18.5 |new_york     |2025-08-10 08:15:30|\n",
      "|uuid-004|rider-D|driver-W|31.9 |new_york     |2025-08-10 11:40:00|\n",
      "|uuid-008|rider-H|driver-S|16.25|new_york     |2025-08-10 15:00:20|\n",
      "|uuid-010|rider-J|driver-Q|20.0 |new_york     |2025-08-10 16:30:00|\n",
      "|uuid-003|rider-C|driver-Z|14.6 |chicago      |2025-08-10 10:05:45|\n",
      "|uuid-006|rider-F|driver-U|19.8 |chicago      |2025-08-10 13:20:35|\n",
      "|uuid-009|rider-I|driver-R|24.35|chicago      |2025-08-10 15:45:50|\n",
      "|uuid-011|rider-K|driver-P|10.5 |chicago      |2025-08-11 10:00:00|\n",
      "|uuid-002|rider-B|driver-Y|50.0 |san_francisco|2025-08-10 09:22:10|\n",
      "|uuid-005|rider-E|driver-V|25.1 |san_francisco|2025-08-10 12:55:15|\n",
      "+--------+-------+--------+-----+-------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"REFRESH TABLE {table_name_cow}\")\n",
    "tripsDF_read = spark.sql(f\"select uuid, rider, driver, fare, city, ts from {table_name_cow}\")\n",
    "tripsDF_read.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7bbb7b2-bdfd-49b6-9127-02e6d0ec5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|tableName        |isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|         |trips_incremental|false      |\n",
      "|         |trips_table_cow  |false      |\n",
      "|         |trips_time_travel|false      |\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"show tables\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f86785-047f-466a-9a23-b7fb443412d8",
   "metadata": {},
   "source": [
    "## Merge-on-Read (MOR) Tables\n",
    "For comparison, let's explore the Merge-on-Read (MOR) table type. In a MOR table, updates are written to a separate log file, which is then merged with the base data files when you read the table.\n",
    "\n",
    "Here's the configuration for our MOR table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0305c335-edee-432d-b8d3-ccee1673dbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_mor = \"trips_table_mor\"\n",
    "base_path = f\"s3a://warehouse/hudi-db\"\n",
    "\n",
    "hudi_conf = {\n",
    "    \"hoodie.table.name\": table_name_mor,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"uuid\",\n",
    "    \"hoodie.datasource.write.table.type\": \"MERGE_ON_READ\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"city\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.write.markers.type\": \"DIRECT\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0def29-610b-4e43-8b58-13ba38d367e1",
   "metadata": {},
   "source": [
    "### Inserting data\n",
    "\n",
    "Let's insert the same initial dataset into our new MOR table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb31c40d-d72c-4f92-b42c-d0d07139923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a Hudi MOR table\n",
    "inputDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"insert\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(f\"{base_path}/{table_name_mor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6f466a-77b5-4a62-98b4-174e0cf997a7",
   "metadata": {},
   "source": [
    "### Reading in Real-time Mode (Default)\n",
    "\n",
    "When you perform a standard read on a MOR table, Hudi automatically merges the base and log files for you. This gives you the most up-to-date, real-time view of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0dabdf5-c114-489d-aa01-40de9adcbac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|                 ts|    uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|  20250813173942960|20250813173942960...|          uuid-002|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 09:22:10|uuid-002|rider-B|driver-Y|22.75|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-005|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 12:55:15|uuid-005|rider-E|driver-V| 25.1|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-007|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 14:10:05|uuid-007|rider-G|driver-T|28.45|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-001|         city=new_york|9f864817-215b-4ae...|2025-08-10 08:15:30|uuid-001|rider-A|driver-X| 18.5|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-004|         city=new_york|9f864817-215b-4ae...|2025-08-10 11:40:00|uuid-004|rider-D|driver-W| 31.9|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-008|         city=new_york|9f864817-215b-4ae...|2025-08-10 15:00:20|uuid-008|rider-H|driver-S|16.25|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-010|         city=new_york|9f864817-215b-4ae...|2025-08-10 16:30:00|uuid-010|rider-J|driver-Q| 20.0|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-003|          city=chicago|05405119-82be-41f...|2025-08-10 10:05:45|uuid-003|rider-C|driver-Z| 14.6|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-006|          city=chicago|05405119-82be-41f...|2025-08-10 13:20:35|uuid-006|rider-F|driver-U| 19.8|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-009|          city=chicago|05405119-82be-41f...|2025-08-10 15:45:50|uuid-009|rider-I|driver-R|24.35|      chicago|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"hudi\").load(f\"{base_path}/{table_name_mor}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276773ff-c99d-4e08-bdf2-667be99d8ef6",
   "metadata": {},
   "source": [
    "### Reading in Read-Optimized Mode\n",
    "Now, let's read the same table in read-optimized mode. This mode is faster because it only reads the base files, but it won't show any recent updates that are still in the log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b06ffb2d-d382-4ddf-8d4c-fb6b92166827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|                 ts|    uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|  20250813173942960|20250813173942960...|          uuid-002|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 09:22:10|uuid-002|rider-B|driver-Y|22.75|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-005|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 12:55:15|uuid-005|rider-E|driver-V| 25.1|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-007|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 14:10:05|uuid-007|rider-G|driver-T|28.45|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-001|         city=new_york|9f864817-215b-4ae...|2025-08-10 08:15:30|uuid-001|rider-A|driver-X| 18.5|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-004|         city=new_york|9f864817-215b-4ae...|2025-08-10 11:40:00|uuid-004|rider-D|driver-W| 31.9|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-008|         city=new_york|9f864817-215b-4ae...|2025-08-10 15:00:20|uuid-008|rider-H|driver-S|16.25|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-010|         city=new_york|9f864817-215b-4ae...|2025-08-10 16:30:00|uuid-010|rider-J|driver-Q| 20.0|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-003|          city=chicago|05405119-82be-41f...|2025-08-10 10:05:45|uuid-003|rider-C|driver-Z| 14.6|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-006|          city=chicago|05405119-82be-41f...|2025-08-10 13:20:35|uuid-006|rider-F|driver-U| 19.8|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-009|          city=chicago|05405119-82be-41f...|2025-08-10 15:45:50|uuid-009|rider-I|driver-R|24.35|      chicago|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.query.type\", \"read_optimized\") \\\n",
    "    .load(f\"{base_path}/{table_name_mor}\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a329ae-7d17-4321-b518-ad9c4bed42e3",
   "metadata": {},
   "source": [
    "### Updating a Record in the MOR table\n",
    "Let's update a record in our MOR table to see how it affects our read modes. We'll find the record for 'driver-W' and double its fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd02accc-5ddb-4118-9840-5c3ad23474f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+----+--------+-------------------+\n",
      "|    uuid|  rider|  driver|fare|    city|                 ts|\n",
      "+--------+-------+--------+----+--------+-------------------+\n",
      "|uuid-004|rider-D|driver-W|63.8|new_york|2025-08-10 11:40:00|\n",
      "+--------+-------+--------+----+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "updatesDF = spark.read.format(\"hudi\").load(f\"{base_path}/{table_name_mor}\").filter(col(\"driver\") == \"driver-W\").withColumn(\"fare\", col(\"fare\") * 2)\n",
    "\n",
    "updatesDF.select(\"uuid\", \"rider\", \"driver\", \"fare\", \"city\", \"ts\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea366650-e657-411a-a937-23a576bf8d8d",
   "metadata": {},
   "source": [
    "Now we perform the upsert. In a MOR table, this update will be written to a log file, separate from the main Parquet data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "479901c9-6a7e-4d01-914c-3ce7a52b2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "updatesDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name_mor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11589029-e02d-46c4-b4d9-f9df11734710",
   "metadata": {},
   "source": [
    "After the update, a real-time read correctly shows the new fare for 'driver-W'. This is because the log files containing our update were merged with the base files during this read operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "55636bea-9e64-4884-b192-904c21cb1e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|                 ts|    uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|  20250813173942960|20250813173942960...|          uuid-002|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 09:22:10|uuid-002|rider-B|driver-Y|22.75|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-005|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 12:55:15|uuid-005|rider-E|driver-V| 25.1|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-007|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 14:10:05|uuid-007|rider-G|driver-T|28.45|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-001|         city=new_york|9f864817-215b-4ae...|2025-08-10 08:15:30|uuid-001|rider-A|driver-X| 18.5|     new_york|\n",
      "|  20250813173944757|20250813173944757...|          uuid-004|         city=new_york|9f864817-215b-4ae...|2025-08-10 11:40:00|uuid-004|rider-D|driver-W| 63.8|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-008|         city=new_york|9f864817-215b-4ae...|2025-08-10 15:00:20|uuid-008|rider-H|driver-S|16.25|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-010|         city=new_york|9f864817-215b-4ae...|2025-08-10 16:30:00|uuid-010|rider-J|driver-Q| 20.0|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-003|          city=chicago|05405119-82be-41f...|2025-08-10 10:05:45|uuid-003|rider-C|driver-Z| 14.6|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-006|          city=chicago|05405119-82be-41f...|2025-08-10 13:20:35|uuid-006|rider-F|driver-U| 19.8|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-009|          city=chicago|05405119-82be-41f...|2025-08-10 15:45:50|uuid-009|rider-I|driver-R|24.35|      chicago|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"hudi\").load(f\"{base_path}/{table_name_mor}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00078ec6-a2bb-4ff1-ae3a-a394ea46c9f9",
   "metadata": {},
   "source": [
    "Finally, a read-optimized query of the same table still shows the old fare for 'driver-W'. This is because the read-optimized query only looks at the base data files and ignores the unmerged update in the log file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eee3febe-32f2-4e5c-9ae9-710878a60b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|                 ts|    uuid|  rider|  driver| fare|         city|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "|  20250813173942960|20250813173942960...|          uuid-002|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 09:22:10|uuid-002|rider-B|driver-Y|22.75|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-005|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 12:55:15|uuid-005|rider-E|driver-V| 25.1|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-007|    city=san_francisco|f4c57bc4-500f-48f...|2025-08-10 14:10:05|uuid-007|rider-G|driver-T|28.45|san_francisco|\n",
      "|  20250813173942960|20250813173942960...|          uuid-001|         city=new_york|9f864817-215b-4ae...|2025-08-10 08:15:30|uuid-001|rider-A|driver-X| 18.5|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-004|         city=new_york|9f864817-215b-4ae...|2025-08-10 11:40:00|uuid-004|rider-D|driver-W| 31.9|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-008|         city=new_york|9f864817-215b-4ae...|2025-08-10 15:00:20|uuid-008|rider-H|driver-S|16.25|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-010|         city=new_york|9f864817-215b-4ae...|2025-08-10 16:30:00|uuid-010|rider-J|driver-Q| 20.0|     new_york|\n",
      "|  20250813173942960|20250813173942960...|          uuid-003|          city=chicago|05405119-82be-41f...|2025-08-10 10:05:45|uuid-003|rider-C|driver-Z| 14.6|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-006|          city=chicago|05405119-82be-41f...|2025-08-10 13:20:35|uuid-006|rider-F|driver-U| 19.8|      chicago|\n",
      "|  20250813173942960|20250813173942960...|          uuid-009|          city=chicago|05405119-82be-41f...|2025-08-10 15:45:50|uuid-009|rider-I|driver-R|24.35|      chicago|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+-------------------+--------+-------+--------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.query.type\", \"read_optimized\") \\\n",
    "    .load(f\"{base_path}/{table_name_mor}\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9947be96-5464-454f-af40-06130057aca4",
   "metadata": {},
   "source": [
    "## Deduplication with Precombine Field\n",
    "\n",
    "A key feature of Hudi is its ability to handle duplicate records automatically. Hudi uses the precombine.field to decide which record to keep when it encounters two or more records with the same recordkey in a single write operation. In our configuration, the precombine.field is set to ts, so Hudi will keep the record with the latest timestamp.\n",
    "\n",
    "Let's create a new DataFrame with some duplicate data to see this in action. We'll add two records with the same uuid (uuid-001) but with different ts values. The second record has a later timestamp and a higher fare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3210dcd2-a969-4518-ae5b-48ec0ad34a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+-------+--------+-----+--------+\n",
      "|ts                 |uuid    |rider  |driver  |fare |city    |\n",
      "+-------------------+--------+-------+--------+-----+--------+\n",
      "|2025-08-10 08:15:30|uuid-001|rider-A|driver-Z|25.5 |new_york|\n",
      "|2025-08-10 17:00:00|uuid-001|rider-A|driver-A|30.0 |new_york|\n",
      "|2025-08-11 07:45:00|uuid-012|rider-L|driver-T|12.25|chicago |\n",
      "+-------------------+--------+-------+--------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "duplicate_data = [\n",
    "    (\"2025-08-10 08:15:30\", \"uuid-001\", \"rider-A\", \"driver-Z\", 25.50, \"new_york\"), # A duplicate record with an old timestamp\n",
    "    (\"2025-08-10 17:00:00\", \"uuid-001\", \"rider-A\", \"driver-A\", 30.00, \"new_york\"), # Another duplicate record with a new timestamp\n",
    "    (\"2025-08-11 07:45:00\", \"uuid-012\", \"rider-L\", \"driver-T\", 12.25, \"chicago\")   # A new record\n",
    "]\n",
    "duplicate_columns = [\"ts\", \"uuid\", \"rider\", \"driver\", \"fare\", \"city\"]\n",
    "duplicatesDF = spark.createDataFrame(duplicate_data).toDF(*duplicate_columns)\n",
    "\n",
    "duplicatesDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e5d51-f037-4fac-b12d-3fbdaaa63237",
   "metadata": {},
   "source": [
    "Now, let's upsert this data into our COW table. Hudi will process the duplicate records for uuid-001 and, based on our precombine.field (ts), it will only keep the record with the later timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d4e14a8-32eb-482d-b6b4-34c72c40d864",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name_cow = \"trips_table_cow\"\n",
    "base_path = f\"s3a://warehouse/hudi-db\"\n",
    "\n",
    "hudi_conf = {\n",
    "    \"hoodie.table.name\": table_name_cow,\n",
    "    \"hoodie.datasource.write.recordkey.field\": \"uuid\",\n",
    "    \"hoodie.datasource.write.table.type\": \"COPY_ON_WRITE\",\n",
    "    \"hoodie.datasource.write.partitionpath.field\": \"city\",\n",
    "    \"hoodie.datasource.write.precombine.field\": \"ts\",\n",
    "    \"hoodie.write.markers.type\": \"DIRECT\",\n",
    "    \"hoodie.table.cdc.enabled\": \"true\",\n",
    "    \"hoodie.datasource.write.hive_style_partitioning\": \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a9af4b03-21bc-4885-b5e5-5f70b4580143",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicatesDF.write \\\n",
    "    .format(\"hudi\") \\\n",
    "    .option(\"hoodie.datasource.write.operation\", \"upsert\") \\\n",
    "    .options(**hudi_conf) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(f\"{base_path}/{table_name_cow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44071ab6-67cf-4cd1-8735-f7e3252bb636",
   "metadata": {},
   "source": [
    "Finally, we'll query the table to see the result. As you can see in the output, only one record for uuid-001 exists, and it's the one with the latest timestamp (2025-08-10 17:00:00). The record with the older timestamp was discarded, and the new record for uuid-012 was successfully inserted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c6a230a-7e1f-4b2b-978f-5abf17c375f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+-----+--------+-------------------+\n",
      "|uuid    |rider  |driver  |fare |city    |ts                 |\n",
      "+--------+-------+--------+-----+--------+-------------------+\n",
      "|uuid-012|rider-L|driver-T|12.25|chicago |2025-08-11 07:45:00|\n",
      "|uuid-001|rider-A|driver-A|30.0 |new_york|2025-08-10 17:00:00|\n",
      "+--------+-------+--------+-----+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"REFRESH TABLE {table_name_cow}\")\n",
    "spark.sql(f\"select uuid, rider, driver, fare, city, ts from {table_name_cow} where uuid = 'uuid-001' or uuid = 'uuid-012'\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ba5f52f-9d8f-4cec-99ea-8cd9d1b3f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils/display_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c9d6a16-2992-4754-be0e-a53f32fbbaf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .dataframe {\n",
       "            border-radius: 0.5rem;\n",
       "            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);\n",
       "            overflow-x: auto;\n",
       "            border: 1px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe th {\n",
       "            background-color: #f1f5f9;\n",
       "            color: #1f2937;\n",
       "            font-weight: 600;\n",
       "            padding: 0.75rem 1.5rem;\n",
       "            text-align: left;\n",
       "            border-bottom: 2px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe td {\n",
       "            padding: 0.75rem 1.5rem;\n",
       "            border-bottom: 1px solid #e2e8f0;\n",
       "        }\n",
       "        .dataframe tr:nth-child(even) {\n",
       "            background-color: #f8fafc;\n",
       "        }\n",
       "        .dataframe tr:hover {\n",
       "            background-color: #e2e8f0;\n",
       "            transition: background-color 0.2s ease-in-out;\n",
       "        }\n",
       "    </style>\n",
       "    <table border=\"1\" class=\"dataframe w-full border-collapse text-sm text-gray-900 dark:text-white\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ts</th>\n",
       "      <th>uuid</th>\n",
       "      <th>rider</th>\n",
       "      <th>driver</th>\n",
       "      <th>fare</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2025-08-10 08:15:30</td>\n",
       "      <td>uuid-001</td>\n",
       "      <td>rider-A</td>\n",
       "      <td>driver-X</td>\n",
       "      <td>18.50</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 09:22:10</td>\n",
       "      <td>uuid-002</td>\n",
       "      <td>rider-B</td>\n",
       "      <td>driver-Y</td>\n",
       "      <td>22.75</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 10:05:45</td>\n",
       "      <td>uuid-003</td>\n",
       "      <td>rider-C</td>\n",
       "      <td>driver-Z</td>\n",
       "      <td>14.60</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 11:40:00</td>\n",
       "      <td>uuid-004</td>\n",
       "      <td>rider-D</td>\n",
       "      <td>driver-W</td>\n",
       "      <td>31.90</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 12:55:15</td>\n",
       "      <td>uuid-005</td>\n",
       "      <td>rider-E</td>\n",
       "      <td>driver-V</td>\n",
       "      <td>25.10</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 13:20:35</td>\n",
       "      <td>uuid-006</td>\n",
       "      <td>rider-F</td>\n",
       "      <td>driver-U</td>\n",
       "      <td>19.80</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 14:10:05</td>\n",
       "      <td>uuid-007</td>\n",
       "      <td>rider-G</td>\n",
       "      <td>driver-T</td>\n",
       "      <td>28.45</td>\n",
       "      <td>san_francisco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 15:00:20</td>\n",
       "      <td>uuid-008</td>\n",
       "      <td>rider-H</td>\n",
       "      <td>driver-S</td>\n",
       "      <td>16.25</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 15:45:50</td>\n",
       "      <td>uuid-009</td>\n",
       "      <td>rider-I</td>\n",
       "      <td>driver-R</td>\n",
       "      <td>24.35</td>\n",
       "      <td>chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2025-08-10 16:30:00</td>\n",
       "      <td>uuid-010</td>\n",
       "      <td>rider-J</td>\n",
       "      <td>driver-Q</td>\n",
       "      <td>20.00</td>\n",
       "      <td>new_york</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(inputDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c35cd8-2632-4974-87b1-2c60e467a8f1",
   "metadata": {},
   "source": [
    "### Deleting a Record in a MOR Table\n",
    "Just like with the upsert, deleting a record in a Merge-on-Read table is handled by writing a new log file, not by rewriting the entire base file. Hudi records the delete action in the log, and the record will appear to be gone in a snapshot/real-time query.\n",
    "\n",
    "First, let's create a DataFrame that contains the record we want to delete. We'll delete the record for rider-E (uuid-005)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbfcb36-db7c-48d0-bd3e-380d99c60264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
